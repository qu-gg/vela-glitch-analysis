<h1 align='center'>Single Pulse Analyses of the Vela Pulsar <br> using Machine Learning Techniques<br><br>
   Vela-Pulsar (<a href='https://academic.oup.com/mnras/article-abstract/509/4/5790/6433657'>MNRAS</a>, <a href='https://arxiv.org/abs/2108.13462'>arXiv</a>)<br>
   Vela-Glitch (<a href=''>MNRAS</a>, <a href=''>arXiv</a>)</h2>
   
   
   
<p>This repository holds the experiments and models as explored in the works, "Vela pulsar: single pulses analysis with machine learning techniques" and "First glitching Pulsars monitoring program at the Argentine Institute of Radioastronomy" as published in <a href="https://academic.oup.com/mnras">MNRAS</a>. We provide training and visualization scripts. Data generated by our calculations or observations are available from the corresponding authors upon reasonable request.</p>

### Overview

We leverage Variational Autoencoders (VAEs) and Self-Organizing Maps (SOMs) to enable individual pulse reconstruction and clustering from the Vela Pulsar (SR B0833-45 / J0835-4510) given daily observations ranging from 1-3 hours and 50-100k pulses. The first works presents the initial presentation of the technique when applied to pulsar data and highlights the results on 4 days of observation from January and March of 2021. Using these techniques, we were able to isolate 'mini-giant' pulses effectively  into clusters and highlight the trend of increasing signal peak amplitude with earlier pulse arrival times, supporting earlier pulsar models suggesting pulsar emitting regions of different heights in thheir magnetospheres.

<p align='center'><img src="https://user-images.githubusercontent.com/32918812/182661125-c86a6805-7bf8-4b53-8f1c-6488a6b103a9.png" alt="framework schematic" )/></p>
<p align='center'>Fig 1. Schematic of the prposed VAE-SOM model used in analysis.</p>

### Repository structure

Here we detail the folder structure of the repository, for convenience. We include a <code>requirements.txt</code> file for packages used.
```
  vela-glitch-analysis/
  │
  ├── README.md                                    # What you're reading right now :^)
  ├── requirements.txt                             # Pip requirements file to enable easy setup
  |
  ├── raw_som_reconstruct.py                       # SOM analysis of the raw signals
  ├── vae_model.py                                 # VAE model implementation
  ├── vae_reconstruct.py                           # SOM analysis of the VAE reconstructions
  ├── vae_train.py                                 # VAE training script
  ├── temporal_cluster_histogram.py                # Plot functions related to signals over time
  |
  ├── data/                                        # Files of the month and day captured
  |   └── <monthDay>/                              
  ├── graphs/                                      # Graphical plots for the given day and antenna
  |   └── <monthDayAntenna>/                       
  ├── models/                                      # Trained PyTorch checkpoints
  │   └── <monthDayAntenna_modelType.torch         
  ├── reconstructed/                               # VAE models' reconstructions of datasets
  │   └── <monthDayAntenna_modelType.torch         
  |── scripts/                                     # Automated training .bat scripts
  │   └── complete_reconstruction_script.bat       
  ├── utils/                                       # Various functions for plotting and model utility
  │   ├── plot_functions.py                        
  |   └── util_functions.py                  
  └──
```

### Data

Data-specific processing details are best read in the acknowledged papers; here we focus on how they're loaded for the VAE and SOM models. Figure 2 includes the neural architecture schematic. To get the relevant window of peak for a dataset, we use a windowing technique where the average index of the max signal value is found across the dataset and a window of 100 before and after timesteps are taken. In the event that the average peak occurs near the boundaries (which is simply a data split in pre-processing consideration), checks are used to truncated one side and extend the other to fit 200 timesteps. 

### Code + Models

We include two modes of analysis, raw-signal analsis and analysis using the reconstructions from the VAE models. Argparse arguments are specific to the data-structure of month/day/antenna but this can easily be swapped out in the <code>np.load()</code> for whatever data you have.

<b>Raw</b>: We simply include <code>raw_som_reconstruct.py</code> to handle loading, clustering, and getting statistics for the raw signals with SOM. 
<b>VAE</b>: The general workflow goes from VAE training (<code>var_train.py</code>) then application of the SOM to get clusters and plots out (<code>var_reconstruct.py</code>). The trained models are saved under <code>models/</code> and their per-day reconstruction files saved under <code>reconstructed/</code>.

The VAE architecture and training script is loosely based on the available project <a href="https://github.com/sksq96/pytorch-vae/blob/master/vae-cnn.ipynb">here</a>, however it was adapted for adaptive pooling and parameter counts relevant to the problem. An analysis with hyperparameter tuning was not performed in-depth, however with the current setup it was fairly robust across days and variations.

<p align='center'><img width="50%" height="50%"  src="https://user-images.githubusercontent.com/32918812/193403514-d912dc2a-a305-4202-ae16-656ce48f24c9.png" alt="neural architecture schematic" )/></p>
<p align='center'>Fig 2. Schematic of the neural network data-input and layer-specific architectures.</p>

### Visualizations

We provide many visualizations with respect to the VAE reconstruction and the SOM clustering analysis, as well as text statistics. An example analysis is provided within <code>graphs/</code>. It is broken down first by dataset, then by raw/vae signals used, and finally the SOM shape considered for analysis. Figure 3 shows an example of one of the visualizations provided.

<p align='center'><img src="https://user-images.githubusercontent.com/32918812/193405956-58843c8c-c05f-4fe4-8086-989fc4ed46d7.png" alt="clustering example" )/></p>
<p align='center'>Fig 3. Example of a 6-cluster SOM on the July 19th dataset.</p>
